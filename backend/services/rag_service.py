from dotenv import load_dotenv
load_dotenv()
import os
import json
import hashlib
from typing import List, Dict, Any
from datetime import datetime
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_distances
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import openai
from models.schemas import ChatResponse
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CSVWatcher(FileSystemEventHandler):
    """CSVæ–‡ä»¶ç›‘å¬å™¨"""
    
    def __init__(self, rag_service):
        self.rag_service = rag_service
        
    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('data_catalog.csv'):
            logger.info(f"æ£€æµ‹åˆ°CSVæ–‡ä»¶å˜æ›´: {event.src_path}")
            self.rag_service.update_vector_db()

class RAGService:
    """RAGæ™ºèƒ½é—®ç­”æœåŠ¡"""
    
    def __init__(self, data_path: str = "../data", vector_db_path: str = "../vector_db"):
        self.data_path = data_path
        self.vector_db_path = vector_db_path
        self.csv_file = os.path.join(data_path, "data_catalog.csv")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(vector_db_path, exist_ok=True)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = None
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
        else:
            logger.warning("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ™ºèƒ½é—®ç­”åŠŸèƒ½å°†å—é™")
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.documents = []
        self.metadata = []
        self.embeddings = None
        self.nn_model = None
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        self.load_vector_db()
        
        # å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨
        self.start_file_watcher()
        
        logger.info(f"å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
    
    def start_file_watcher(self):
        """å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨"""
        try:
            self.observer = Observer()
            event_handler = CSVWatcher(self)
            self.observer.schedule(event_handler, self.data_path, recursive=False)
            self.observer.start()
            logger.info("æ–‡ä»¶ç›‘å¬å™¨å·²å¯åŠ¨ï¼Œå°†è‡ªåŠ¨åŒæ­¥ç¼–ç›®æ•°æ®å˜æ›´")
        except Exception as e:
            logger.error(f"å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨å¤±è´¥: {e}")
    
    def stop_file_watcher(self):
        """åœæ­¢æ–‡ä»¶ç›‘å¬å™¨"""
        if hasattr(self, 'observer'):
            self.observer.stop()
            self.observer.join()
    
    def load_vector_db(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            documents_file = os.path.join(self.vector_db_path, "documents.json")
            embeddings_file = os.path.join(self.vector_db_path, "embeddings.npy")
            nn_model_file = os.path.join(self.vector_db_path, "nn_model.pkl")
            
            if os.path.exists(documents_file) and os.path.exists(embeddings_file):
                # åŠ è½½æ–‡æ¡£å’Œå…ƒæ•°æ®
                with open(documents_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.documents = data['documents']
                    self.metadata = data['metadata']
                
                # åŠ è½½åµŒå…¥å‘é‡
                self.embeddings = np.load(embeddings_file)
                
                # åŠ è½½æˆ–åˆ›å»ºæœ€è¿‘é‚»æ¨¡å‹
                if os.path.exists(nn_model_file):
                    import pickle
                    with open(nn_model_file, 'rb') as f:
                        self.nn_model = pickle.load(f)
                else:
                    self._build_nn_model()
            else:
                # é¦–æ¬¡æ„å»ºå‘é‡æ•°æ®åº“
                self.update_vector_db()
                
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            self.documents = []
            self.metadata = []
            self.embeddings = None
            self.nn_model = None
    
    def _build_nn_model(self):
        """æ„å»ºæœ€è¿‘é‚»æ¨¡å‹"""
        if self.embeddings is not None and len(self.embeddings) > 0:
            self.nn_model = NearestNeighbors(
                n_neighbors=min(5, len(self.embeddings)),
                metric='cosine'
            )
            self.nn_model.fit(self.embeddings)
            
            # ä¿å­˜æ¨¡å‹
            nn_model_file = os.path.join(self.vector_db_path, "nn_model.pkl")
            import pickle
            with open(nn_model_file, 'wb') as f:
                pickle.dump(self.nn_model, f)
    
    def update_vector_db(self):
        """æ›´æ–°å‘é‡æ•°æ®åº“"""
        try:
            if not os.path.exists(self.csv_file):
                logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {self.csv_file}")
                return
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(self.csv_file)
            
            # æ„å»ºæ–‡æ¡£
            documents = []
            metadata = []
            
            for _, row in df.iterrows():
                # æ„å»ºæ–‡æ¡£æ–‡æœ¬
                doc_text = self._build_document_text(row)
                documents.append(doc_text)
                
                # æ„å»ºå…ƒæ•°æ®
                meta = {
                    'table_name_en': row['table_name_en'],
                    'resource_name': row['resource_name'],
                    'domain_category': row['domain_category'],
                    'organization_name': row['organization_name'],
                    'layer': row['layer']
                }
                metadata.append(meta)
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            if documents:
                embeddings = self.embedding_model.encode(documents)
                
                # ä¿å­˜æ•°æ®
                self.documents = documents
                self.metadata = metadata
                self.embeddings = embeddings
                
                # æ„å»ºæœ€è¿‘é‚»æ¨¡å‹
                self._build_nn_model()
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                self._save_vector_db()
                
                logger.info(f"å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            logger.error(f"æ›´æ–°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
    
    def _build_document_text(self, row):
        """æ„å»ºæ–‡æ¡£æ–‡æœ¬"""
        try:
            # è§£æå­—æ®µä¿¡æ¯
            fields_info = ""
            if pd.notna(row['fields_json']) and row['fields_json']:
                try:
                    fields = json.loads(row['fields_json'])
                    field_names = [f['field_name_cn'] for f in fields if f.get('field_name_cn')]
                    fields_info = ", ".join(field_names)
                except:
                    fields_info = ""
            
            # è§£ææ¥æºè¡¨ä¿¡æ¯
            source_tables_info = ""
            if pd.notna(row.get('source_tables')) and row['source_tables']:
                try:
                    source_tables = json.loads(row['source_tables'])
                    if source_tables:
                        source_table_names = []
                        for table in source_tables:
                            table_en = table.get('table_name_en', '')
                            table_cn = table.get('table_name_cn', '')
                            if table_en and table_cn:
                                source_table_names.append(f"{table_en}({table_cn})")
                            elif table_en:
                                source_table_names.append(table_en)
                        source_tables_info = ", ".join(source_table_names)
                except:
                    source_tables_info = ""
            
            # æ„å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"""è¡¨å: {row['table_name_en']}
èµ„æºåç§°: {row['resource_name']}
èµ„æºæ‘˜è¦: {row['resource_summary']}
æ•°æ®åˆ†å±‚: {row['layer']}
é¢†åŸŸåˆ†ç±»: {row['domain_category']}
ç»„ç»‡æœºæ„: {row['organization_name']}
ç³»ç»Ÿåç§°: {row['irs_system_name']}
å­—æ®µ: {fields_info}"""

            # æ·»åŠ æ¥æºè¡¨ä¿¡æ¯
            if source_tables_info:
                doc_text += f"\næ¥æºè¡¨: {source_tables_info}"
            
            # å¦‚æœæ˜¯åŠ å·¥è¡¨ï¼Œæ·»åŠ åŠ å·¥é€»è¾‘
            if row.get('is_processed') and pd.notna(row.get('processing_logic')):
                doc_text += f"\nåŠ å·¥é€»è¾‘: {row['processing_logic']}"
            
            return doc_text
            
        except Exception as e:
            logger.error(f"æ„å»ºæ–‡æ¡£æ–‡æœ¬å¤±è´¥: {e}")
            return f"è¡¨å: {row.get('table_name_en', 'unknown')}"
    
    def _save_vector_db(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        try:
            # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            documents_file = os.path.join(self.vector_db_path, "documents.json")
            data = {
                'documents': self.documents,
                'metadata': self.metadata,
                'last_update': datetime.now().isoformat()
            }
            with open(documents_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åµŒå…¥å‘é‡
            embeddings_file = os.path.join(self.vector_db_path, "embeddings.npy")
            np.save(embeddings_file, self.embeddings)
            
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£ - æ··åˆæœç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.nn_model or len(self.documents) == 0:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query])
        
        # æœç´¢æœ€è¿‘é‚»
        distances, indices = self.nn_model.kneighbors(query_embedding, n_neighbors=min(top_k, len(self.documents)))
        
        results = []
        query_lower = query.lower()
        
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦ (cosine distance -> cosine similarity)
            similarity = 1 - distance
            
            # å…³é”®è¯åŒ¹é…åŠ æƒ
            doc_text = self.documents[idx].lower()
            table_name = self.metadata[idx]['table_name_en'].lower()
            keyword_boost = 0.0
            
            # ç‰¹æ®Šå¤„ç†ï¼šè¡¨åç²¾ç¡®åŒ¹é…è·å¾—æœ€é«˜æƒé‡
            if table_name in query_lower:
                keyword_boost += 1.0  # è¡¨åç²¾ç¡®åŒ¹é…ï¼Œæœ€é«˜ä¼˜å…ˆçº§
            
            # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
            if query_lower in doc_text:
                keyword_boost += 0.3  # ç²¾ç¡®åŒ¹é…åŠ æƒ
            
            # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
            query_words = query_lower.split()
            for word in query_words:
                if len(word) > 1 and word in doc_text:
                    keyword_boost += 0.1  # å•è¯åŒ¹é…åŠ æƒ
            
            # ç‰¹æ®Šå…³é”®è¯é¢å¤–åŠ æƒ
            special_keywords = {
                'å…¬ç§¯é‡‘': ['å…¬ç§¯é‡‘', 'gjj'],
                'ä½æˆ¿': ['ä½æˆ¿', 'æˆ¿å±‹'],
                'ç¤¾ä¿': ['ç¤¾ä¿', 'ç¤¾ä¼šä¿é™©'],
                'åŒ»ç–—': ['åŒ»ç–—', 'åŒ»é™¢'],
                'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡'],
                'ä¼ä¸š': ['ä¼ä¸š', 'å…¬å¸'],
                'ç¯ä¿': ['ç¯ä¿', 'ç¯å¢ƒ'],
                'å­—æ®µ': ['å­—æ®µ', 'field', 'åˆ—', 'å±æ€§'],
                'è¡¨ç»“æ„': ['è¡¨ç»“æ„', 'ç»“æ„', 'schema']
            }
            
            for key, synonyms in special_keywords.items():
                if key in query_lower:
                    for synonym in synonyms:
                        if synonym in doc_text:
                            keyword_boost += 0.2  # åŒä¹‰è¯åŒ¹é…åŠ æƒ
                            break
            
            # æœ€ç»ˆåˆ†æ•° = å‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯åŠ æƒ
            final_score = similarity + keyword_boost
            
            if final_score > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                results.append({
                    'document': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'score': float(final_score),
                    'vector_score': float(similarity),
                    'keyword_boost': float(keyword_boost),
                    'rank': i + 1
                })
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°é‡æ–°æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        
        # æ›´æ–°æ’å
        for i, result in enumerate(results):
            result['rank'] = i + 1
        
        return results
    
    def chat(self, question: str) -> ChatResponse:
        """
        æ™ºèƒ½é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ChatResponse: é—®ç­”ç»“æœ
        """
        try:
            # æœç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.search(question, top_k=5)
            
            if not search_results:
                return ChatResponse(
                    answer="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ•°æ®èµ„æºä¿¡æ¯ã€‚è¯·å°è¯•å…¶ä»–é—®é¢˜æˆ–æ£€æŸ¥ç¼–ç›®æ•°æ®æ˜¯å¦å®Œæ•´ã€‚",
                    sources=[]
                )
            
            # å¦‚æœæ²¡æœ‰OpenAIå®¢æˆ·ç«¯ï¼Œè¿”å›åŸºç¡€æœç´¢ç»“æœ
            if not self.openai_client:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å­—æ®µç›¸å…³æŸ¥è¯¢
                is_field_query = any(keyword in question.lower() for keyword in ['å­—æ®µ', 'field', 'åˆ—', 'å±æ€§', 'è¡¨ç»“æ„', 'ç»“æ„'])
                
                # æ„å»ºåŸºç¡€å›ç­”
                if is_field_query and search_results:
                    # å­—æ®µæŸ¥è¯¢çš„ç‰¹æ®Šå¤„ç†
                    top_result = search_results[0]
                    meta = top_result['metadata']
                    doc = top_result['document']
                    
                    answer = f"æ ¹æ®ç¼–ç›®ä¿¡æ¯ï¼Œ{meta['table_name_en']}è¡¨çš„è¯¦ç»†ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n"
                    answer += f"ğŸ“‹ **è¡¨åŸºæœ¬ä¿¡æ¯**\n"
                    answer += f"- è¡¨åï¼š{meta['table_name_en']}\n"
                    answer += f"- èµ„æºåç§°ï¼š{meta['resource_name']}\n"
                    answer += f"- é¢†åŸŸåˆ†ç±»ï¼š{meta['domain_category']}\n"
                    answer += f"- ç»„ç»‡æœºæ„ï¼š{meta['organization_name']}\n"
                    answer += f"- æ•°æ®åˆ†å±‚ï¼š{meta['layer']}\n\n"
                    
                    # æå–å­—æ®µä¿¡æ¯
                    if "å­—æ®µ:" in doc:
                        fields_text = doc.split("å­—æ®µ:")[1].split("\n")[0].strip()
                        if fields_text and fields_text != "":
                            fields_list = [f.strip() for f in fields_text.split(",") if f.strip()]
                            if fields_list:
                                answer += f"ğŸ“ **å­—æ®µä¿¡æ¯** (å…±{len(fields_list)}ä¸ªå­—æ®µ)ï¼š\n"
                                for i, field in enumerate(fields_list, 1):
                                    answer += f"{i}. {field}\n"
                            else:
                                answer += "ğŸ“ **å­—æ®µä¿¡æ¯**ï¼šæš‚æ— è¯¦ç»†å­—æ®µä¿¡æ¯\n"
                        else:
                            answer += "ğŸ“ **å­—æ®µä¿¡æ¯**ï¼šæš‚æ— è¯¦ç»†å­—æ®µä¿¡æ¯\n"
                    else:
                        answer += "ğŸ“ **å­—æ®µä¿¡æ¯**ï¼šæš‚æ— è¯¦ç»†å­—æ®µä¿¡æ¯\n"
                else:
                    # æ™®é€šæŸ¥è¯¢çš„å¤„ç†
                    answer = f"æ ¹æ®æœç´¢ç»“æœï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³çš„æ•°æ®èµ„æºï¼š\n\n"
                    for i, result in enumerate(search_results[:3], 1):
                        meta = result['metadata']
                        answer += f"{i}. {meta['table_name_en']}\n   - èµ„æºåç§°: {meta['resource_name']}\n   - é¢†åŸŸåˆ†ç±»: {meta['domain_category']}\n   - ç»„ç»‡æœºæ„: {meta['organization_name']}\n\n"
        
                sources = [result['metadata']['table_name_en'] for result in search_results[:3]]
                return ChatResponse(answer=answer, sources=sources)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(search_results)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_chat_prompt(question, context)
            
            # è°ƒç”¨OpenAI API
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç›®å½•åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å’Œäº†è§£æ•°æ®èµ„æºä¿¡æ¯ã€‚è¯·æ ¹æ®æä¾›çš„ç¼–ç›®ä¿¡æ¯ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            
            # æå–å¼•ç”¨æ¥æº
            sources = [result['metadata']['table_name_en'] for result in search_results[:3]]
            
            return ChatResponse(answer=answer, sources=sources)
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½é—®ç­”å¤±è´¥: {e}")
            return ChatResponse(
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                sources=[]
            )
    
    def _build_context(self, search_results: List[Dict]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context = "ç›¸å…³æ•°æ®èµ„æºç¼–ç›®ä¿¡æ¯ï¼š\n\n"
        for i, result in enumerate(search_results[:3], 1):
            context += f"æ–‡æ¡£{i}ï¼š\n{result['document']}\n\n"
        
        return context
    
    def _build_chat_prompt(self, question: str, context: str) -> str:
        """æ„å»ºèŠå¤©æç¤ºè¯"""
        prompt = f"""åŸºäºä»¥ä¸‹æ•°æ®ç›®å½•ç¼–ç›®ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æ ¹æ®æä¾›çš„ç¼–ç›®ä¿¡æ¯ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ•°æ®èµ„æºã€‚"""
        
        return prompt
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": len(self.documents),
            "last_update": datetime.now().isoformat() if self.documents else None
        }
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, 'observer'):
            self.stop_file_watcher()